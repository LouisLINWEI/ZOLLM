2025-10-13 16:31:54,580 - INFO - Sample train set 1500/67349
2025-10-13 16:31:54,580 - INFO - ... including dev set 500 samples
2025-10-13 16:31:54,580 - INFO - Loading model with FP32...
23
2025-10-13 16:31:59,224 - INFO - Done with 4.64s
2025-10-13 16:31:59,695 - INFO - Dev samples: 500
2025-10-13 16:31:59,695 - INFO - Train samples: 1000
2025-10-13 16:31:59,696 - INFO - Eval sample length is 872
2025-10-13 16:31:59,696 - INFO - Tokenizing training samples...
2025-10-13 16:32:00,905 - INFO - Done with 1.21s
2025-10-13 16:32:00,908 - INFO - Checkpoint detected, resuming training at result/zo_sgd-SST2-0-opt-1.3b-OPTIM_ft-STEP20000-adamw-LR1e-05-constant-ZOEPS1e-08-Q1/checkpoint-500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.
/home/wlin23/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Traceback (most recent call last):
  File "/home/wlin23/ZO-LLM/zo-bench/run.py", line 727, in <module>
    main()
  File "/home/wlin23/ZO-LLM/zo-bench/run.py", line 679, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/wlin23/ZO-LLM/zo-bench/run.py", line 564, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/wlin23/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/wlin23/ZO-LLM/zo-bench/trainer.py", line 312, in _inner_training_loop
    self._load_optimizer_and_scheduler(resume_from_checkpoint)
  File "/home/wlin23/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 2492, in _load_optimizer_and_scheduler
    self.optimizer.load_state_dict(
  File "/home/wlin23/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
    return disable_fn(*args, **kwargs)
  File "/home/wlin23/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/home/wlin23/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 866, in load_state_dict
    raise ValueError(
ValueError: loaded state dict has a different number of parameter groups