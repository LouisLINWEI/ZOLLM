2025-10-13 19:25:06,004 - INFO - Sample train set 96/67349
2025-10-13 19:25:06,005 - INFO - ... including dev set 32 samples
2025-10-13 19:25:06,005 - INFO - Loading model with FP32...
23
2025-10-13 19:25:11,041 - INFO - Done with 5.04s
2025-10-13 19:25:11,402 - INFO - Dev samples: 32
2025-10-13 19:25:11,402 - INFO - Train samples: 64
2025-10-13 19:25:11,402 - INFO - Eval sample length is 64
2025-10-13 19:25:11,402 - INFO - Tokenizing training samples...
2025-10-13 19:25:11,471 - INFO - Done with 0.07s
/home/wlin23/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-10-13 19:25:11,480 - INFO - ***** Running training *****
2025-10-13 19:25:11,480 - INFO -   Num examples = 64
2025-10-13 19:25:11,480 - INFO -   Num Epochs = 4
2025-10-13 19:25:11,480 - INFO -   Instantaneous batch size per device = 4
2025-10-13 19:25:11,480 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 4
2025-10-13 19:25:11,480 - INFO -   Gradient Accumulation steps = 1
2025-10-13 19:25:11,480 - INFO -   Total optimization steps = 50
2025-10-13 19:25:11,481 - INFO -   Number of trainable parameters = 1315758080
  0%|          | 0/50 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
{'peak_mem': 15.097169399261475, 'step_consumption': 637.6807689666748, 'epoch': 0.06}
 10%|█         | 5/50 [00:02<00:18,  2.45it/s]
{'peak_mem': 16.060410976409912, 'step_consumption': 397.4039554595947, 'epoch': 0.12}
{'peak_mem': 16.060410976409912, 'step_consumption': 391.190767288208, 'epoch': 0.19}
{'peak_mem': 16.060410976409912, 'step_consumption': 389.8484706878662, 'epoch': 0.25}
{'loss': 0.4246, 'learning_rate': 1e-05, 'epoch': 0.31}

 20%|██        | 10/50 [00:03<00:14,  2.69it/s]
{'peak_mem': 16.060410976409912, 'step_consumption': 302.0479679107666, 'epoch': 0.38}
{'peak_mem': 16.060410976409912, 'step_consumption': 372.60890007019043, 'epoch': 0.44}
{'peak_mem': 16.060410976409912, 'step_consumption': 328.6607265472412, 'epoch': 0.5}
{'peak_mem': 16.060410976409912, 'step_consumption': 372.4822998046875, 'epoch': 0.56}
{'loss': 0.7835, 'learning_rate': 1e-05, 'epoch': 0.62}

 32%|███▏      | 16/50 [00:06<00:11,  3.02it/s]
{'peak_mem': 16.060410976409912, 'step_consumption': 460.4837894439697, 'epoch': 0.69}
{'peak_mem': 16.060410976409912, 'step_consumption': 337.3587131500244, 'epoch': 0.75}
{'peak_mem': 16.060410976409912, 'step_consumption': 327.14295387268066, 'epoch': 0.81}
{'peak_mem': 16.060410976409912, 'step_consumption': 327.33988761901855, 'epoch': 0.88}
{'loss': 0.9952, 'learning_rate': 1e-05, 'epoch': 0.94}
{'peak_mem': 16.060410976409912, 'step_consumption': 327.27718353271484, 'epoch': 0.94}
{'peak_mem': 16.060410976409912, 'step_consumption': 295.1042652130127, 'epoch': 1.0}

 44%|████▍     | 22/50 [00:08<00:10,  2.74it/s]
{'peak_mem': 16.060410976409912, 'step_consumption': 391.02816581726074, 'epoch': 1.06}
{'peak_mem': 16.060410976409912, 'step_consumption': 326.88212394714355, 'epoch': 1.12}
{'peak_mem': 16.060410976409912, 'step_consumption': 372.2496032714844, 'epoch': 1.19}
{'loss': 0.4135, 'learning_rate': 1e-05, 'epoch': 1.25}
{'peak_mem': 16.060410976409912, 'step_consumption': 391.21150970458984, 'epoch': 1.25}
{'peak_mem': 16.060410976409912, 'step_consumption': 327.5902271270752, 'epoch': 1.31}
 48%|████▊     | 24/50 [00:08<00:09,  2.81it/s]2025-10-13 19:25:20,444 - INFO - There are 0 training samples and 32 validation samples
100%|██████████| 32/32 [00:01<00:00, 26.81it/s]
2025-10-13 19:25:21,639 - INFO - There are 0 training samples and 64 validation samples
  5%|▍         | 3/64 [00:00<00:02, 25.35it/s]
{'peak_mem': 16.060410976409912, 'step_consumption': 372.4627494812012, 'epoch': 1.44}

 50%|█████     | 25/50 [00:13<00:40,  1.64s/it]
 50%|█████     | 4/8 [00:00<00:00,  8.30it/s]
{'test_acc': 0.59375, 'val_acc': 0.4375, 'epoch': 1.5}
{'peak_mem': 16.060410976409912, 'step_consumption': 327.00419425964355, 'epoch': 1.5}

{'loss': 1.0183, 'learning_rate': 1e-05, 'epoch': 1.56}
{'eval_loss': 0.9725793600082397, 'eval_runtime': 1.2013, 'eval_samples_per_second': 53.275, 'eval_steps_per_second': 6.659, 'epoch': 1.56}
{'peak_mem': 16.138780117034912, 'step_consumption': 1531.1546325683594, 'epoch': 1.56}
{'peak_mem': 16.138780117034912, 'step_consumption': 459.39087867736816, 'epoch': 1.62}
{'peak_mem': 16.138780117034912, 'step_consumption': 326.22218132019043, 'epoch': 1.69}

 68%|██████▊   | 34/50 [00:18<00:06,  2.29it/s]
{'peak_mem': 16.138780117034912, 'step_consumption': 371.9916343688965, 'epoch': 1.81}
{'loss': 0.7087, 'learning_rate': 1e-05, 'epoch': 1.88}
{'peak_mem': 16.138780117034912, 'step_consumption': 389.8000717163086, 'epoch': 1.88}
{'peak_mem': 16.138780117034912, 'step_consumption': 371.5035915374756, 'epoch': 1.94}
{'peak_mem': 16.138780117034912, 'step_consumption': 371.2043762207031, 'epoch': 2.0}
-------------------------- Training Epoch 2 --------------------------
{'peak_mem': 16.138780117034912, 'step_consumption': 325.6375789642334, 'epoch': 2.06}

 80%|████████  | 40/50 [00:20<00:03,  2.73it/s]
{'loss': 0.7062, 'learning_rate': 1e-05, 'epoch': 2.19}
{'peak_mem': 16.138780117034912, 'step_consumption': 389.5993232727051, 'epoch': 2.19}
{'peak_mem': 16.138780117034912, 'step_consumption': 388.96894454956055, 'epoch': 2.25}
{'peak_mem': 16.138780117034912, 'step_consumption': 371.4911937713623, 'epoch': 2.31}
{'peak_mem': 16.138780117034912, 'step_consumption': 371.0603713989258, 'epoch': 2.38}

 90%|█████████ | 45/50 [00:22<00:01,  2.69it/s]
{'loss': 0.6242, 'learning_rate': 1e-05, 'epoch': 2.5}
{'peak_mem': 16.138780117034912, 'step_consumption': 371.5512752532959, 'epoch': 2.5}
{'peak_mem': 16.138780117034912, 'step_consumption': 459.37347412109375, 'epoch': 2.56}
{'peak_mem': 16.138780117034912, 'step_consumption': 326.1904716491699, 'epoch': 2.62}
{'peak_mem': 16.138780117034912, 'step_consumption': 390.14744758605957, 'epoch': 2.69}
{'peak_mem': 16.138780117034912, 'step_consumption': 326.02548599243164, 'epoch': 2.75}
{'loss': 0.7316, 'learning_rate': 1e-05, 'epoch': 2.81}
 98%|█████████▊| 49/50 [00:23<00:00,  2.81it/s]2025-10-13 19:25:35,137 - INFO - There are 0 training samples and 32 validation samples
 56%|█████▋    | 18/32 [00:00<00:00, 27.91it/s]
{'peak_mem': 16.138780117034912, 'step_consumption': 371.40440940856934, 'epoch': 2.88}
{'peak_mem': 16.138780117034912, 'step_consumption': 326.9999027252197, 'epoch': 2.94}
{'peak_mem': 16.138780117034912, 'step_consumption': 325.96611976623535, 'epoch': 3.0}
-------------------------- Training Epoch 3 --------------------------
2025-10-13 19:25:36,297 - INFO - There are 0 training samples and 64 validation samples
100%|██████████| 50/50 [00:27<00:00,  1.48s/it]
 25%|██▌       | 2/8 [00:00<00:00, 15.78it/s]
{'test_acc': 0.59375, 'val_acc': 0.4375, 'epoch': 3.06}
{'peak_mem': 16.138780117034912, 'step_consumption': 371.6387748718262, 'epoch': 3.06}
{'loss': 0.9544, 'learning_rate': 1e-05, 'epoch': 3.12}
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████| 50/50 [00:29<00:00,  1.72it/s]
2025-10-13 19:25:40,502 - INFO - There are 0 training samples and 64 validation samples
Evaluating on the Test Set:  56%|█████▋    | 36/64 [00:01<00:01, 26.45it/s]
{'eval_loss': 0.9720144867897034, 'eval_runtime': 1.2046, 'eval_samples_per_second': 53.129, 'eval_steps_per_second': 6.641, 'epoch': 3.12}
Evaluating on the Test Set: 100%|██████████| 64/64 [00:02<00:00, 26.02it/s]
2025-10-13 19:25:42,962 - INFO - There are 0 training samples and 32 validation samples
Evaluating on the Validation Set: 100%|██████████| 32/32 [00:01<00:00, 27.92it/s]
2025-10-13 19:25:44,109 - INFO - {'accuracy': 0.59375, 'test_accuracy': 0.59375, 'val_accuracy': 0.4375}
2025-10-13 19:25:44,109 - INFO - ===== Train set 0 =====
2025-10-13 19:25:44,109 - INFO - {'accuracy': 0.59375, 'test_accuracy': 0.59375, 'val_accuracy': 0.4375}
WARNING: Removing everything at end: result/agzo-SST2-0-opt-1.3b-OPTIM_ft-STEP50-adamw-LR1e-05-constant-ZOEPS1e-08-Q1-AGZO_PI3
deleted folders:  []